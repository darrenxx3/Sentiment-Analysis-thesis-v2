{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilation BERT (DISTILBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure NVIDIA GPU as Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce GTX 1650\n",
      "GPU: NVIDIA GeForce GTX 1650 is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Configure device: Set the device configuration to GPU using the torch.device class in PyTorch:\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Selected Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mohon donk kembalikan tanda biru hitam saat li...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kenapa sekarang untuk transfer ke blu bca ada ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oke terimakasih</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tolong pihak developer untuk update hp saya ga...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ok</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  score  sentiment\n",
       "0  mohon donk kembalikan tanda biru hitam saat li...      1          0\n",
       "1  kenapa sekarang untuk transfer ke blu bca ada ...      2          0\n",
       "2                                    oke terimakasih      5          2\n",
       "3  tolong pihak developer untuk update hp saya ga...      5          2\n",
       "4                                                 ok      5          2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bca_preprocessed_data.csv', delimiter=',')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and clean missing data pt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content      61\n",
       "score         0\n",
       "sentiment     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bca_clean_again = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content      0\n",
       "score        0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bca_clean_again.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13939, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bca_clean_again.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Balance when SplittingðŸª“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split features (content) and labels (sentiment)\n",
    "\n",
    "X = bca_clean_again['content']\n",
    "y = bca_clean_again['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing:\n",
      "sentiment\n",
      "0    7443\n",
      "2    5429\n",
      "1    1067\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check dataset balance and apply oversampling for \"Neutral\" class if needed\n",
    "print(\"Before balancing:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Melakukan balancing data karena kelas 0 mempunyai values yang sedikit dan dapat berpengaruh kepada performa yang tidak baik menggunakan Oversampling: duplikasi class minoritas untuk menyeimbangkan dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling technique\n",
    "from sklearn.utils import resample\n",
    "\n",
    "bca_clean_again = pd.concat([X, y], axis=1)\n",
    "\n",
    "#separating each class\n",
    "\n",
    "class_0 = bca_clean_again[bca_clean_again['sentiment'] == 0]\n",
    "class_1 = bca_clean_again[bca_clean_again['sentiment'] == 1]\n",
    "class_2 = bca_clean_again[bca_clean_again['sentiment'] == 2]\n",
    "\n",
    "#oversampling neutral class 1 with majority class 2\n",
    "\n",
    "class_1_oversampled = resample(class_1, replace=True, n_samples=len(class_2), random_state=42)\n",
    "\n",
    "#combine all class\n",
    "bca_clean_again = pd.concat([class_0, class_1_oversampled, class_2])\n",
    "\n",
    "#shuffling the data\n",
    "bca_clean_again = bca_clean_again.sample(frac=1, random_state=42)\n",
    "\n",
    "X = bca_clean_again['content']\n",
    "y = bca_clean_again['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing:\n",
      "sentiment\n",
      "0    7443\n",
      "1    5429\n",
      "2    5429\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check dataset balance and apply oversampling for \"Neutral\" class if needed\n",
    "print(\"After balancing:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting DataðŸª“ðŸª“ðŸª“ , 80:20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "#                                                     random_state=42, stratify=y)\n",
    "\n",
    "# #split 20% test+validation into 10%test dan 10%validation\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "#                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "#split 20% test+validation into 10%test dan 10%validation\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, \n",
    "                                               test_size=0.5, random_state=42,stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14640\n",
      "Validation set size: 1830\n",
      "Test set size: 1831\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Tokenizer with DistilBert-base-Multilingual-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformer library\n",
    "from transformers import AutoTokenizer, DistilBertModel, DistilBertForSequenceClassification, DistilBertTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(tokenizer, texts, max_len=128):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation = True,\n",
    "        return_tensors = 'pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DistilSentiment_dataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokens = tokenize_data(self.tokenizer, [text], self.max_len)\n",
    "        return{\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens['attention_mask'].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#create datasets\n",
    "train_dataset = DistilSentiment_dataset(X_train.values, y_train.values, tokenizer)\n",
    "validation_dataset = DistilSentiment_dataset(X_val.values, y_val.values, tokenizer)\n",
    "test_dataset = DistilSentiment_dataset(X_test.values, y_test.values, tokenizer)\n",
    "\n",
    "#create dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Sentimentclassifier(nn.Module):\n",
    "    # construct class\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Sentimentclassifier, self).__init__()\n",
    "        self.distilbert = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\",\n",
    "                                                          num_labels=num_classes,\n",
    "                                                          id2label={0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\"},\n",
    "                                                          label2id={\"NEGATIVE\": 0, \"NEUTRAL\": 1, \"POSITIVE\": 2}\n",
    "        )\n",
    "    \n",
    "    #forward propagation class\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.distilbert(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1830/1830 [08:06<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 1428.3848, Accuracy: 0.6423\n",
      "Validation Loss: 139.2475, Validation Accuracy: 0.7497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1830/1830 [08:08<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 912.1678, Accuracy: 0.8130\n",
      "Validation Loss: 102.0414, Validation Accuracy: 0.8311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1830/1830 [08:05<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 552.3362, Accuracy: 0.8915\n",
      "Validation Loss: 92.1715, Validation Accuracy: 0.8579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1830/1830 [08:17<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 404.6836, Accuracy: 0.9227\n",
      "Validation Loss: 80.9454, Validation Accuracy: 0.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1830/1830 [08:16<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 293.3378, Accuracy: 0.9438\n",
      "Validation Loss: 88.0576, Validation Accuracy: 0.8770\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "#setup the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Sentimentclassifier().to(device)\n",
    "\n",
    "#set optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        #training\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1} - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "        #validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "train_model(model, train_loader,val_loader, optimizer, loss_fn, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       745\n",
      "           1       0.93      0.96      0.94       543\n",
      "           2       0.85      0.77      0.81       543\n",
      "\n",
      "    accuracy                           0.87      1831\n",
      "   macro avg       0.87      0.87      0.87      1831\n",
      "weighted avg       0.87      0.87      0.87      1831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        print(classification_report(all_labels, all_preds))\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save = \"distilepoch3.pt\"\n",
    "torch.save(model.state_dict(), model_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\vocab.txt',\n",
       " 'tokenizer\\\\added_tokens.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save tokenizer\n",
    "tokenizer_save = \"tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/d:/SEMESTER 8/thesis-v2/distilepoch3.pt'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Darren\\anaconda3\\envs\\pytorch_nvidia\\lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    404\u001b[0m         path_or_repo_id,\n\u001b[0;32m    405\u001b[0m         filename,\n\u001b[0;32m    406\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    407\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    408\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    409\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    410\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    411\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    412\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    413\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    414\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    415\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    416\u001b[0m     )\n\u001b[0;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Darren\\anaconda3\\envs\\pytorch_nvidia\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[0;32m    108\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Darren\\anaconda3\\envs\\pytorch_nvidia\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/d:/SEMESTER 8/thesis-v2/distilepoch3.pt'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\SEMESTER 8\\thesis-v2\\modeling.ipynb Cell 43\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SEMESTER%208/thesis-v2/modeling.ipynb#X60sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/d:/SEMESTER 8/thesis-v2/tokenizer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SEMESTER%208/thesis-v2/modeling.ipynb#X60sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SEMESTER%208/thesis-v2/modeling.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m DistilBertForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(model_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SEMESTER%208/thesis-v2/modeling.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(tokenizer_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEMESTER%208/thesis-v2/modeling.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Move the model to the appropriate device (GPU or CPU)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Darren\\anaconda3\\envs\\pytorch_nvidia\\lib\\site-packages\\transformers\\modeling_utils.py:3518\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3515\u001b[0m \u001b[39mif\u001b[39;00m commit_hash \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3516\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   3517\u001b[0m         \u001b[39m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m-> 3518\u001b[0m         resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   3519\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   3520\u001b[0m             CONFIG_NAME,\n\u001b[0;32m   3521\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   3522\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   3523\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   3524\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   3525\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   3526\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   3527\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   3528\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m   3529\u001b[0m             _raise_exceptions_for_gated_repo\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   3530\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   3531\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   3532\u001b[0m         )\n\u001b[0;32m   3533\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m   3534\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Darren\\anaconda3\\envs\\pytorch_nvidia\\lib\\site-packages\\transformers\\utils\\hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere was a specific connection error when trying to load \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    468\u001b[0m \u001b[39mexcept\u001b[39;00m HFValidationError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 469\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    470\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncorrect path_or_model_id: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    471\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: '/d:/SEMESTER 8/thesis-v2/distilepoch3.pt'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Paths where the model and tokenizer were saved\n",
    "model_path = \"/d:/SEMESTER 8/thesis-v2/distilepoch3.pt\"\n",
    "tokenizer_path = \"/d:/SEMESTER 8/thesis-v2/tokenizer\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, text):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    attention_mask = tokens['attention_mask'].to(device)\n",
    "\n",
    "    #perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilites = torch.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "    max_prob, prediction = torch.max(probabilites, dim=1)\n",
    "\n",
    "    label_map={0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\"}\n",
    "    result = {\n",
    "        \"label\": label_map[prediction.item()],\n",
    "        \"score\": max_prob.item()\n",
    "    }\n",
    "    # return label_map[prediction.item()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(model, tokenizer, text, neutral_threshold=0.4):\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    attention_mask = tokens['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # Custom neutral threshold\n",
    "    max_prob, prediction = torch.max(probabilities, dim=1)\n",
    "    if max_prob < neutral_threshold:\n",
    "        return {\"label\": \"NEUTRAL\", \"score\": max_prob.item()}\n",
    "\n",
    "    label_map = {0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\"}\n",
    "    return {\"label\": label_map[prediction.item()], \"score\": max_prob.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: POSITIVE, 'Score': 0.6531272530555725\n"
     ]
    }
   ],
   "source": [
    "text = \"Transfer dana berjalan lancar, tetapi menambahkan rekening baru agak rumit\"\n",
    "predicted_sentiment = predict_sentiment(model, tokenizer, text)\n",
    "\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment['label']}, 'Score': {predicted_sentiment['score']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nvidia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
